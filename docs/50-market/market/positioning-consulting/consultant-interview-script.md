# Consultant Discovery Interview Script

*Purpose: Validate assumptions before building consulting-specific features*
*Target: 5-7 independent/boutique consultants*
*Duration: 30-45 minutes*

---

## Interview Goals

| Assumption to Validate | Confidence Before | Target After |
|------------------------|-------------------|--------------|
| Discovery synthesis is painful | MEDIUM | HIGH |
| 30-50% non-billable time on synthesis | LOW | MEDIUM-HIGH |
| Would pay $99-249/mo for solution | LOW | MEDIUM-HIGH |
| "Decision defensibility" resonates | LOW | MEDIUM |
| Evidence traceability matters | MEDIUM | HIGH |
| Current tools are inadequate | MEDIUM | HIGH |

---

## Before the Interview

**Recruit criteria:**
- Independent consultant OR works at boutique firm (5-50 people)
- Does stakeholder/discovery interviews as part of their work
- Management, IT, strategy, or operations consulting
- Has completed 3+ discovery projects in past year

**NOT looking for:**
- Enterprise consultants at Big 4/MBB (different workflow)
- Coaches or solo practitioners without stakeholder interviews
- Consultants who only do implementation (no discovery)

**Scheduling note:**
> "We're researching how consultants handle stakeholder discovery interviews. Looking for 30 minutes to understand your workflow. No pitch, just learning."

---

## Interview Script

### Opening (2 min)

> "Thanks for taking the time. I'm researching how consultants handle stakeholder interviews and synthesis. There's no pitch today—I'm just trying to understand your workflow and pain points. Everything you share helps us understand if we're solving a real problem."

> "Is it okay if I record this for notes? It won't be shared."

> "Quick background: what type of consulting do you do, and how long have you been doing it?"

---

### Section 1: Current Workflow (10 min)

**Goal:** Understand their actual process, not what they think we want to hear.

#### 1.1 Project Structure

> "Think about a recent discovery project where you interviewed multiple stakeholders. Can you walk me through what that looked like?"

**Listen for:**
- Number of stakeholders interviewed
- Interview duration
- Who conducts interviews (them vs. junior staff)
- Timeline from first interview to deliverable

**Probe:**
> "How many stakeholders did you interview?"
> "How long was each interview?"
> "Did you conduct them yourself or delegate?"

#### 1.2 Capture Method

> "How do you capture what's said during those interviews?"

**Listen for:**
- Note-taking (manual vs. typed)
- Recording (yes/no, what tool)
- Transcription (yes/no, what tool)
- Real-time vs. after-the-fact

**Probe:**
> "Do you record interviews? Why or why not?"
> "If you record, do you transcribe? How?"
> "What happens to the notes/recordings after?"

#### 1.3 Synthesis Process

> "After you've done all the interviews, how do you pull it all together?"

**Listen for:**
- Manual copy-paste
- Spreadsheets
- Mind maps / Miro
- AI tools (ChatGPT, etc.)
- Time spent on synthesis

**Critical probe:**
> "Roughly how many hours do you spend synthesizing after all interviews are done?"
> "Is that time billable to the client?"

**Note:** This validates/invalidates the "30-50% non-billable" assumption.

---

### Section 2: Pain Points (10 min)

**Goal:** Identify real pain, not hypothetical pain.

#### 2.1 What's Hard

> "What's the most frustrating part of the discovery process?"

**Listen for (don't prompt):**
- Synthesis time
- Finding quotes
- Conflicting stakeholder views
- Remembering who said what
- Creating deliverables
- Rework after client feedback

**Probe:**
> "Can you give me a specific example of when that was really painful?"

#### 2.2 Stakeholder Conflicts

> "When different stakeholders say conflicting things, how do you handle that?"

**Listen for:**
- Manual tracking
- Memory/intuition
- Explicit documentation
- Client presentation of conflicts

**Probe:**
> "How do you present those conflicts to the client?"
> "Do you track who said what by role?"

#### 2.3 Evidence & Traceability

> "When you make a recommendation in your final deliverable, can you trace it back to who said it?"

**Listen for:**
- "Sort of" / "If I remember"
- Explicit quote tracking
- No traceability at all
- Client ever asks for sources

**Critical probe:**
> "Has a client ever challenged a recommendation and asked 'where did this come from?'"
> "How did you handle that?"

**Note:** This validates/invalidates the "decision defensibility" positioning.

---

### Section 3: Current Tools (5 min)

**Goal:** Understand what they've tried and what's missing.

#### 3.1 Tool Stack

> "What tools do you use for discovery interviews and synthesis today?"

**Listen for:**
- Otter / Fireflies / Fathom (transcription)
- Notion / Google Docs (notes)
- Miro / Mural (synthesis)
- Dovetail / Looppanel (unlikely, but ask)
- ChatGPT / Claude (synthesis assist)
- Nothing / manual

**Probe:**
> "Have you tried using AI tools like ChatGPT to help with synthesis?"
> "What worked? What didn't?"

#### 3.2 What's Missing

> "If you could wave a magic wand and fix one thing about your discovery workflow, what would it be?"

**Listen for their priority, don't suggest ours.**

---

### Section 4: Concept Testing (8 min)

**Goal:** Test reaction to our value prop without pitching.

#### 4.1 Problem Framing

> "Let me describe what we're thinking about building and get your reaction."

> "Imagine a tool where you upload or record stakeholder interviews, and it automatically:
> - Transcribes everything
> - Identifies themes across all interviews
> - Shows you which stakeholders agree or disagree on each theme
> - Lets you click any theme to see exactly who said it, with the quote
> - Exports a 'discovery brief' you can edit into your deliverable"

**Ask:**
> "What's your initial reaction?"
> "What would be most valuable? Least valuable?"
> "What's missing?"

#### 4.2 Positioning Test

> "We've been describing this as helping consultants 'turn stakeholder conversations into defensible decisions.' Does that resonate? How would you describe it?"

**Listen for:**
- Does "defensible" land?
- What language do THEY use?
- Alternative framings

#### 4.3 Deliverable Preference

> "Would you want the tool to draft a full SOW or proposal? Or would you prefer it gives you the building blocks and you write the SOW yourself?"

**Listen for:**
- SOW is too risky / firm-specific
- Building blocks preferred
- Actually wants full SOW draft
- Depends on project type

**Note:** This validates/invalidates ChatGPT's critique about SOW being premature.

---

### Section 5: Willingness to Pay (5 min)

**Goal:** Get real pricing signal, not hypothetical.

#### 5.1 Current Spend

> "Roughly how much do you spend per month on tools for your consulting work? Just a ballpark."

**Listen for:** Anchoring on what they already pay.

#### 5.2 Value Framing

> "You mentioned synthesis takes about [X hours] per project. If this tool cut that in half, what would that be worth to you?"

**Let them do the math in their head.**

#### 5.3 Direct Pricing

> "If this existed today and worked well, would you pay $99/month for it?"

**Probe based on response:**
- If yes: "What would make it a no-brainer?"
- If no: "What would it need to do to be worth that?"
- If hesitation: "What's the hesitation?"

**Alternative framing:**
> "Would you rather pay per-project ($X per project) or monthly subscription?"

---

### Section 6: Closing (2 min)

> "This has been really helpful. A few quick closing questions:"

> "If we built this, would you be willing to try an early version and give feedback?"

> "Is there anyone else you'd recommend we talk to?"

> "Any questions for me?"

**Thank them. Offer to share findings if interested.**

---

## Post-Interview Scoring

Rate each interview on key assumptions:

| Assumption | Score (1-5) | Evidence/Quote |
|------------|-------------|----------------|
| Discovery synthesis is painful | | |
| Significant non-billable time | | |
| Current tools inadequate | | |
| Evidence traceability matters | | |
| "Defensible decisions" resonates | | |
| Would pay $99/mo | | |
| Prefers building blocks over SOW | | |
| Would try early version | | |

---

## Red Flags to Watch For

| Signal | What It Means |
|--------|---------------|
| "I just use Google Docs and it's fine" | Pain may not be severe enough |
| "My junior staff handles synthesis" | User ≠ buyer problem |
| "I'd never record client interviews" | Privacy/trust barrier |
| "ChatGPT does this for me already" | Commoditization risk |
| "$99/mo is too much for a tool" | Pricing mismatch |
| "I only do 2-3 discovery projects a year" | Low frequency = low value |

---

## Green Flags to Watch For

| Signal | What It Means |
|--------|---------------|
| "I spend 2 days just pulling themes together" | High pain, clear ROI |
| "Clients always ask 'who said that?'" | Traceability is valued |
| "I've tried 3 tools and none work" | Active problem-solver |
| "I'd pay more if it saved me a day" | Price is not the issue |
| "Can I try it on my next project?" | Ready buyer |

---

## Synthesis Template (After 5+ Interviews)

### Pain Validation
- **Confirmed pains:** [list]
- **Weaker than expected:** [list]
- **Unexpected pains:** [list]

### Workflow Insights
- **Average synthesis time:** X hours
- **% non-billable:** X%
- **Most common tools:** [list]
- **AI adoption:** [low/medium/high]

### Positioning Validation
- **"Defensible decisions" reaction:** [resonated / confused / meh]
- **Better language from interviews:** [quotes]

### Pricing Validation
- **$99/mo reaction:** [X of Y positive]
- **Preferred model:** [subscription / per-project / usage]
- **Price sensitivity drivers:** [list]

### Product Implications
- **Must-have features:** [list]
- **Nice-to-have features:** [list]
- **Don't build:** [list]

### Go/No-Go Recommendation
Based on [N] interviews:
- [ ] Strong signal - proceed with confidence
- [ ] Mixed signal - narrow scope or pivot
- [ ] Weak signal - reconsider segment

---

*Run 5-7 interviews before making product decisions. Update confidence levels in consulting-critical-analysis.md after synthesis.*
